{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos la data cruda proveída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo steam_games\n",
    "steam_games = \"steam_games.json.gz\"\n",
    "\n",
    "try:\n",
    "    data = []\n",
    "    with gzip.open(steam_games, 'rt') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al analizar una línea del archivo JSON: {e}\")\n",
    "\n",
    "    # Creamos un DataFrame a partir de la lista de objetos JSON obtenida\n",
    "    df_steam_games = pd.DataFrame(data)\n",
    "\n",
    "    # Trabajamos con df_steam_games como un DataFrame de Pandas\n",
    "    print(df_steam_games.head())  # Muestra las primeras filas del DataFrame\n",
    "except FileNotFoundError:\n",
    "    print(f\"El archivo {steam_games} no se encontró.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "\n",
    "# Guardamos el dataframe obtenido con compresión parquet\n",
    "steam_games.to_parquet('../data/df_steam_games.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo australian_user_reviews\n",
    "\n",
    "data_list = []\n",
    "file = 'australian_user_reviews.json'\n",
    "with open(file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Usar ast.literal_eval para convertir la línea en un diccionario\n",
    "            json_data = ast.literal_eval(line)\n",
    "            data_list.append(json_data)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error en la línea: {line}\")\n",
    "            continue\n",
    "df_user_reviews = pd.DataFrame(data_list)\n",
    "\n",
    "# Creo un df con el archivo review y accedo a la columna que me interesa desanidar.\n",
    "df_reviewsOpen = df_user_reviews.explode('reviews') \n",
    "\n",
    "# Creo un nuevo df donde esta la info desanidada en columnas\n",
    "dfreviewsOpen = pd.concat([df_reviewsOpen.drop(['reviews'], axis=1), df_reviewsOpen['reviews'].apply(pd.Series)], axis=1) \n",
    "\n",
    "# Guardamos el dataframe obtenido con compresión parquet\n",
    "df_reviewsOpen.to_parquet('../data/df_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo australian_users_items\n",
    "\n",
    "data_list = []\n",
    "file_path = 'australian_users_items.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Usar ast.literal_eval para convertir la línea en un diccionario\n",
    "            json_data = ast.literal_eval(line)\n",
    "            data_list.append(json_data)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error en la línea: {line}\")\n",
    "            continue\n",
    "df_user_items = pd.DataFrame(data_list)\n",
    "\n",
    "# Creo un df con el archivo review y accedo a la columna que me interesa desanidar.\n",
    "df_items = df_user_items.explode('items')\n",
    "\n",
    "# Creo un nuevo df donde esta la info desanidada en columnas\n",
    "df_items = pd.concat([df_user_items.drop(['items'], axis=1), df_user_items['items'].apply(pd.Series)], axis=1) \n",
    "\n",
    "# Guardamos el dataframe obtenido con compresión parquet\n",
    "df_items.to_parquet('../data/df_items.parquet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
